{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "RNN的出现解决了NPLM的几个问题，比如窗口固定和不能很好的处理长距离依赖关系。     \n",
    "\n",
    "RNN可以看作是一个具有\"记忆\"的神经网络，其基本原理是通过循环来传递隐藏状态信息，从而实现对序列数据的建模。       \n",
    "\n",
    "RNN的基本组成为输入层、隐藏层和输出层。在每一个时间步，RNN会通过当前输入和上一个时间步的隐藏状态来计算更新这个时间步的隐藏状态，这个隐藏状态同时会用来计算当前的输出和更新下一个时间步的隐藏状态，同时RNN具有参数共享的特征，在不同的时间步，RNN使用相同的权重矩阵和偏置。RNN基本单元和按照时间展开的结构如图：\n",
    "\n",
    "<img src=\"./images/RNN.png\" alt=\"img\" style=\"zoom: 50%;\" />\n",
    "\n",
    "RNN采用BPTT(基于时间的反向传播)算法进行训练, 也就是将梯度沿着时间步反向传播，从输出层一直传播到到输入层。因为RNN层使用相同的权重，因此最终权重梯度是各个RNN层的权重梯度之和。\n",
    "\n",
    "需要注意的是RNN也有有些局限性，比如在训练过程中会遇到梯度消失和梯度爆炸的问题，很难学习长距离依赖关系，因此也就出现了基于门控机制的LSTM和GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN的具体实现\n",
    "整体与NPLM非常相似，只是模型结构有差别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. 构建实验语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表长度 7\n",
      "{'讨厌': 0, '我': 1, '爱': 2, '挨打': 3, '爸爸': 4, '玩具': 5, '喜欢': 6}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"我 喜欢 玩具\", \"我 爱 爸爸\", \"我 讨厌 挨打\"] \n",
    "# 创建词汇表\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "# index -> word\n",
    "index_to_word = { index: word for index,word in enumerate(word_list)}\n",
    "word_to_index = { word: index for index,word in enumerate(word_list)}\n",
    "print(\"词汇表长度\",len(word_list))\n",
    "print(word_to_index)\n",
    "voc_size = len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. 生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印训练数据 tensor([[1, 2],\n",
      "        [1, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 从语料库中生成批处理数据，一批一批的喂入网络进行训练\n",
    "import torch\n",
    "import random\n",
    "batch_size = 2\n",
    "def make_batch(batch_size=2):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    sample_sentences = random.sample(sentences,batch_size) # 通过random随机选择数据\n",
    "    # print(sample_sentences)\n",
    "    for sentence in sample_sentences:\n",
    "        # 通过空格分隔词汇\n",
    "        split_sentences = sentence.split()\n",
    "        input = [word_to_index[ele] for ele in split_sentences[:-1]] # 将前n-1个词汇的索引作为输入\n",
    "        target = word_to_index[split_sentences[-1]] # 目标词汇为最后一个词\n",
    "        input_batch.append(input)\n",
    "        target_batch.append(target)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "    return input_batch, target_batch\n",
    "input_batch, targe_batch = make_batch(batch_size)\n",
    "print(\"打印训练数据\", input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. 构建RNN模型(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.C = nn.Embedding(voc_size,embedding_size)\n",
    "        # 用lstm层替代第一个线性层, batch_first=True表示输入的数据的第一个维度为batch_size\n",
    "        self.lstm = nn.LSTM(embedding_size, n_hidden, batch_first=True)\n",
    "        self.linear = nn.Linear(n_hidden, voc_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # 输入X的形状为[batch_size, n_step]\n",
    "        X = self.C(X) # 通过emdedding后变为:[batch_size, n_step, embedding_size]\n",
    "        lstm_out,_ = self.lstm(X) # lstm_out的形状为[batch_size, n_step, n_hidden]\n",
    "        output = self.linear(lstm_out[:,-1,:]) # 输出的形状为[batch_size, voc_size]\n",
    "        return output\n",
    "        # output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RNN 模型结构： RNN(\n",
      "  (C): Embedding(7, 2)\n",
      "  (lstm): LSTM(2, 2, batch_first=True)\n",
      "  (linear): Linear(in_features=2, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_step = 2 # 时间步数，表示每个输入序列的长度，也就是上下文长度 \n",
    "n_hidden = 2 # 隐藏层大小\n",
    "embedding_size = 2 # 词嵌入大小\n",
    "model = RNN() # 创建神经概率语言模型实例\n",
    "print(' RNN 模型结构：', model) # 打印模型的结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.000000\n",
      "Epoch: 2000 cost = 0.000000\n",
      "Epoch: 3000 cost = 0.000000\n",
      "Epoch: 4000 cost = 0.000000\n",
      "Epoch: 5000 cost = 0.000000\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim # 导入优化器模块\n",
    "criterion = nn.CrossEntropyLoss() # 定义损失函数为交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1) # 定义优化器为 Adam，学习率为 0.1\n",
    "# 训练模型\n",
    "for epoch in range(5000): # 设置训练迭代次数\n",
    "   optimizer.zero_grad() # 清除优化器的梯度\n",
    "   input_batch, target_batch = make_batch() # 创建输入和目标批处理数据\n",
    "   output = model(input_batch) # 将输入数据传入模型，得到输出结果\n",
    "   loss = criterion(output, target_batch) # 计算损失值\n",
    "   if (epoch + 1) % 1000 == 0: # 每 1000 次迭代，打印损失值\n",
    "     print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "   loss.backward() # 反向传播计算梯度\n",
    "   optimizer.step() # 更新模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5. 预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 5])\n",
      "tensor([3, 5])\n",
      "['我', '讨厌'] -> 挨打\n",
      "['我', '喜欢'] -> 玩具\n"
     ]
    }
   ],
   "source": [
    "# 进行预测\n",
    "input_strs = [['我', '讨厌'], ['我', '喜欢']]  # 需要预测的输入序列\n",
    "# 将输入序列转换为对应的索引\n",
    "input_indices = [[word_to_index[word] for word in seq] for seq in input_strs]\n",
    "# 将输入序列的索引转换为张量\n",
    "input_batch = torch.LongTensor(input_indices) \n",
    "# 对输入序列进行预测，取输出中概率最大的类别\n",
    "predict = model(input_batch).data.max(1)[1]  \n",
    "# 将预测结果的索引转换为对应的词\n",
    "print(predict)\n",
    "print(predict.squeeze())\n",
    "predict_strs = [index_to_word[n.item()] for n in predict.squeeze()]  \n",
    "for input_seq, pred in zip(input_strs, predict_strs):\n",
    "   print(input_seq, '->', pred)  # 打印输入序列和预测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN基本实例的介绍\n",
    "来源于源码处的demo,介绍下每个参数及输出的意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n",
      "torch.Size([2, 3, 20])\n",
      "torch.Size([2, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.LSTM(10, 20, 2) # batch_first = false, 因此输入数据的第一个维度为序列的长度即n_step\n",
    "\"\"\"\n",
    "10 : 输入特征的维度\n",
    "20 : 隐藏状态的维度大小, 也就是LSTM单元中隐藏状态大小为20\n",
    "2 : 双层LSTM\n",
    "\"\"\"\n",
    "input = torch.randn(5, 3, 10)\n",
    "\"\"\"\n",
    "5 : 输入序列的长度, 也就是时间步数为5\n",
    "3 : 批次大小, 也就是每个时间步同时处理的序列个数为3\n",
    "10 : 输入特征的维度大小隐\n",
    "\"\"\"\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "\"\"\"\n",
    "初始化模型的隐藏状态和细胞状态\n",
    "2 : lstm的层数\n",
    "3 : 批次大小\n",
    "20 : 隐藏状态的维度\n",
    "\"\"\"\n",
    "output, (hn,cn) = rnn(input, (h0, c0))\n",
    "print(output.shape) # 模型最终输出, 包含了在每个时间步的每个batch的输出(隐藏状态(ht))\n",
    "print(hn.shape) \n",
    "print(cn.shape)\n",
    "# hn和cn表示每个batch在最后一个时间步的隐藏状态和细胞状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN存在的问题\n",
    "\n",
    "- 顺序计算: 每个时间步的数据是顺序计算的, 也就是当前时间步必须在上一个时间步计算完以后才能计算, 限制了网络的并行计算能力, 降低计算效率和速度\n",
    "- 长距离依赖问题: 尽管GRU和LSTM通过门控机制有效了改善了这个问题，但是在序列非常长时，仍然无法完成捕捉到序列中的长距离依赖问题\n",
    "- 扩展性: 在涉及到大规模数据时, 模型的可扩展性较弱。随着序列长度的增加, 计算复杂性会变高, 导致训练时间过长\n",
    "- 在NLP落地方面: 模型表达能力不足, 缺乏大规模数据, 优化算法发展不足\n",
    "  \n",
    "以上问题最终会在注意力机制和transformer的出现之后尘埃落定!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
