{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch function\n",
    "整理学习过程中使用到的pytorch中计算张量及向量的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 转置函数：transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8365,  0.1253,  0.3040,  1.3478],\n",
      "        [-0.6326,  0.0199,  0.7644,  0.3058],\n",
      "        [ 0.7165,  0.7973,  0.5126, -0.0642]])\n",
      "tensor([[ 1.8365, -0.6326,  0.7165],\n",
      "        [ 0.1253,  0.0199,  0.7973],\n",
      "        [ 0.3040,  0.7644,  0.5126],\n",
      "        [ 1.3478,  0.3058, -0.0642]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# transpose -> 输入有两个参数，即针对哪两个维度进行转置\n",
    "x = torch.randn(3, 4)\n",
    "print(x)\n",
    "print(x.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 矩阵相乘：bmm和matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4149,  0.3860, -0.4946, -0.5904, -0.7603],\n",
      "         [ 0.4151, -0.2205,  1.3586,  1.3353,  0.9625],\n",
      "         [ 2.2139, -1.9542, -1.1125, -0.1435, -1.2413]],\n",
      "\n",
      "        [[-0.6122, -2.2527, -2.1380,  3.7162, -1.4753],\n",
      "         [ 0.2149,  1.5300, -0.2037, -2.0867,  0.2021],\n",
      "         [-1.4561,  0.5424, -0.9549, -1.6903, -0.9102]]])\n",
      "tensor([[[-0.4149,  0.3860, -0.4946, -0.5904, -0.7603],\n",
      "         [ 0.4151, -0.2205,  1.3586,  1.3353,  0.9625],\n",
      "         [ 2.2139, -1.9542, -1.1125, -0.1435, -1.2413]],\n",
      "\n",
      "        [[-0.6122, -2.2527, -2.1380,  3.7162, -1.4753],\n",
      "         [ 0.2149,  1.5300, -0.2037, -2.0867,  0.2021],\n",
      "         [-1.4561,  0.5424, -0.9549, -1.6903, -0.9102]]])\n"
     ]
    }
   ],
   "source": [
    "# 1.bmm函数(批量矩阵矩阵乘法) batch_size不参与计算\n",
    "# input: (((batch_size , M, N),(batch_size, N, P)) -> output: (batch_size, M, P))\n",
    "x1 = torch.randn(2, 3, 4)\n",
    "x2 = torch.randn(2, 4, 5)\n",
    "x3 = torch.bmm(x1, x2)\n",
    "print(x3) # (2, 3, 5)\n",
    "\n",
    "# 2.matual函数 更为常用的矩阵相乘函数(矩阵与矩阵, 向量与矩阵) \n",
    "# matual会根据输入矩阵类型来自动判断执行哪种类型的矩阵乘法\n",
    "print(torch.matmul(x1, x2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 归一化：softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9635, -0.5754, -1.0809],\n",
      "        [ 0.4179,  2.0399,  0.4207]])\n",
      "tensor([[0.8876, 0.0701, 0.0423],\n",
      "        [0.1415, 0.7166, 0.1419]])\n"
     ]
    }
   ],
   "source": [
    "# softmax函数对数据进行缩放到0和1之间，且每一行的和为1\n",
    "import torch.nn.functional as F\n",
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "x1 = F.softmax(x, dim=-1)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 矩阵拼接：cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3381,  0.5526, -1.5070, -0.8956],\n",
      "        [ 2.0884,  0.0223, -0.3020,  0.5948],\n",
      "        [-1.7742, -0.3091, -0.0607,  1.8561]])\n",
      "tensor([[-0.2869,  1.0878],\n",
      "        [-0.6471, -0.5091],\n",
      "        [-0.6910, -0.7526]])\n",
      "tensor([[-0.3381,  0.5526, -1.5070, -0.8956, -0.2869,  1.0878],\n",
      "        [ 2.0884,  0.0223, -0.3020,  0.5948, -0.6471, -0.5091],\n",
      "        [-1.7742, -0.3091, -0.0607,  1.8561, -0.6910, -0.7526]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x1 = torch.randn(3, 4)\n",
    "x2 = torch.randn(3, 2)\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(torch.cat((x1,x2), dim=1)) # (3,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 矩阵缩放：view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 对output和target进行view操作后再计算loss的必要性\n",
    "output = torch.randn(2, 3, 20) # batch_size, n_step, vec_size\n",
    "target = torch.randn(2, 3) # batch_size, n_step\n",
    "\n",
    "# 相当于将所有batch的数据展开(堆叠), 然后计算loss\n",
    "output = output.view(-1, 20) # (6, 20)\n",
    "target = target.view(-1) # 6  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
