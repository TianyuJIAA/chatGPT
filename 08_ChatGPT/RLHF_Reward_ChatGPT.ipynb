{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强化学习\n",
    "\n",
    "基于人类反馈的强化学习是ChatGPT中重要的一环，当然整个ChatGPT的训练工程(训练及调优)也是复杂和神秘的。其主要的训练步骤可能有以下几个步骤组成:  \n",
    "\n",
    "step0. 预训练大模型GPT-3.5  \n",
    "step1. 从Prompt数据库中抽样，并在GPT-3.5基础上进行有监督学习微调模型(SFT)\n",
    "step2. 标注人员对有监督微调模型给出的答案进行排序，并分配一个分数，然后使用这些数据训练出具有人类偏好的奖励模型(Reward Model)RM\n",
    "step3. 继续训练模型，并通过RM对模型的输出进行打分再将结果反馈给模型，通过PPO算法进一步优化模型\n",
    "step4. 循环执行上面的步骤，自我优化\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
