{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 书中预处理数据部分的代码有问题，所以重新创建\n",
    "from torchtext.datasets import WikiText2 # 导入WikiText2\n",
    "from torchtext.data.utils import get_tokenizer # 导入Tokenizer分词工具\n",
    "from torchtext.vocab import build_vocab_from_iterator # 导入Vocabulary工具\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\") # 定义数据预处理所需的tokenizer\n",
    "\n",
    "train_path ='data/train.txt' # 加载WikiText2数据集的训练部分\n",
    "valid_path ='data/valid.txt'  # 加载WikiText2数据集的验证部分\n",
    "\n",
    "# 定义一个生成器函数，用于将数据集中的文本转换为tokens\n",
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            yield tokenizer(line)\n",
    "\n",
    "# 创建词汇表，包括特殊tokens：\"<pad>\", \"<sos>\", \"<eos>\"\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_path), \n",
    "                                  specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "\n",
    "# 打印词汇表信息\n",
    "print(\"词汇表大小:\", len(vocab))\n",
    "print(\"词汇示例(word to index):\", \n",
    "      {word: vocab[word] for word in [\"<pad>\", \"<sos>\", \"<eos>\", \"the\", \"apple\"]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
