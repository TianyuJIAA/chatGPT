{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq with attention\n",
    "\n",
    "前面已经学习了Seq2Seq和注意力机制，现在把注意力机制引入到基本的Seq2Seq模型中。  \n",
    "点积注意力公式中的x1在Seq2Seq中为解码器在各个时间步的隐藏状态，形状为(batch_size, seq_len1, feature_dim)；x2在Seq2Seq中则为编码器在各个时间步的隐藏状态，形状为(batch_size, seq_len1, feature_dim)，所以此时可以解释为解码器会在关注了编码器后重新生成一个x1(上下文向量)，解码器利用这个信息可以生成更准确的输出序列。  \n",
    "\n",
    "<img src=\"./images/seq2seq_with_attention.png\" alt=\"examples\" style=\"zoom:34%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seq2seq with attention的训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. 构建语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 句子数量： 5\n",
      " 中文词汇表大小： 18\n",
      " 英文词汇表大小： 20\n",
      " 中文词汇到索引的字典： {'学习': 0, '神经网络': 1, '深度学习': 2, '世界': 3, '强大': 4, '处理': 5, '喜欢': 6, '我': 7, '爱': 8, '语言': 9, '非常': 10, '复杂': 11, '很': 12, '改变': 13, '人工智能': 14, '小冰': 15, '咖哥': 16, '自然': 17}\n",
      " 英文词汇到索引的字典： {'the': 0, 'likes': 1, 'XiaoBing': 2, 'changed': 3, 'studying': 4, 'are': 5, 'AI': 6, 'love': 7, 'complex': 8, '<sos>': 9, 'DL': 10, 'NLP': 11, 'I': 12, 'KaGe': 13, 'powerful': 14, 'so': 15, '<eos>': 16, 'world': 17, 'is': 18, 'Neural-Nets': 19}\n"
     ]
    }
   ],
   "source": [
    "# 1.训练数据 每一组数据包含输入序列、解码器输入以及目标输出(解码器的输入是Teacher Forcing)\n",
    "sentences = [\n",
    "    ['咖哥 喜欢 小冰', '<sos> KaGe likes XiaoBing', 'KaGe likes XiaoBing <eos>'],\n",
    "    ['我 爱 学习 人工智能', '<sos> I love studying AI', 'I love studying AI <eos>'],\n",
    "    ['深度学习 改变 世界', '<sos> DL changed the world', 'DL changed the world <eos>'],\n",
    "    ['自然 语言 处理 很 强大', '<sos> NLP is so powerful', 'NLP is so powerful <eos>'],\n",
    "    ['神经网络 非常 复杂', '<sos> Neural-Nets are complex', 'Neural-Nets are complex <eos>']]\n",
    "\n",
    "# 2.初始化中英文词汇表\n",
    "word_list_cn, word_list_en = [], []\n",
    "for s in sentences:\n",
    "    word_list_cn.extend(s[0].split())\n",
    "    word_list_en.extend(s[1].split())\n",
    "    word_list_en.extend(s[2].split())\n",
    "\n",
    "# 对词汇表进行去重\n",
    "word_list_cn = list(set(word_list_cn))\n",
    "word_list_en = list(set(word_list_en))\n",
    "\n",
    "# 分别构建单词到索引的映射\n",
    "word2idx_cn = {word: idx for idx,word in enumerate(word_list_cn)}\n",
    "word2idx_en = {word: idx for idx,word in enumerate(word_list_en)}\n",
    "\n",
    "idx2word_cn = {idx: word for idx,word in enumerate(word_list_cn)}\n",
    "idx2word_en = {idx: word for idx,word in enumerate(word_list_en)}\n",
    "\n",
    "# 计算词汇表的大小\n",
    "voc_size_cn = len(word_list_cn)\n",
    "voc_size_en = len(word_list_en)\n",
    "print(\" 句子数量：\", len(sentences)) # 打印句子数\n",
    "print(\" 中文词汇表大小：\", voc_size_cn) # 打印中文词汇表大小\n",
    "print(\" 英文词汇表大小：\", voc_size_en) # 打印英文词汇表大小\n",
    "print(\" 中文词汇到索引的字典：\", word2idx_cn) # 打印中文词汇到索引的字典\n",
    "print(\" 英文词汇到索引的字典：\", word2idx_en) # 打印英文词汇到索引的字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. 生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 原始句子： ['咖哥 喜欢 小冰', '<sos> KaGe likes XiaoBing', 'KaGe likes XiaoBing <eos>']\n",
      " 编码器输入张量的形状： torch.Size([1, 3])\n",
      " 解码器输入张量的形状： torch.Size([1, 4])\n",
      " 目标张量的形状： torch.Size([1, 4])\n",
      " 编码器输入张量： tensor([[16,  6, 15]])\n",
      " 解码器输入张量： tensor([[ 9, 13,  1,  2]])\n",
      " 目标张量： tensor([[13,  1,  2, 16]])\n",
      "tensor([13,  1,  2, 16])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# 定义生成训练数据的函数\n",
    "# 之所以没有batch是因为输入序列的长度不一致，如果将会数据变成batch那么需要padding等操作保证输入序列长度一致\n",
    "# 否则torch.LongTensor(encoder_input)会报错\n",
    "def make_data(sentences):\n",
    "    selected_sentence = random.choice(sentences)\n",
    "    # np.array([[1,2,3]]) -> shape[1,3]多维数据, 行数可以看作是batch_size\n",
    "    encoder_input = np.array([[word2idx_cn[s] for s in selected_sentence[0].split()]])\n",
    "    decoder_input = np.array([[word2idx_en[s] for s in selected_sentence[1].split()]])\n",
    "    target = np.array([[word2idx_en[s] for s in selected_sentence[2].split()]])\n",
    "\n",
    "    encoder_input = torch.LongTensor(encoder_input)\n",
    "    decoder_input = torch.LongTensor(decoder_input)\n",
    "    target = torch.LongTensor(target)\n",
    "\n",
    "    return encoder_input, decoder_input, target\n",
    "\n",
    "encoder_input, decoder_input, target = make_data(sentences)\n",
    "\n",
    "# 打印choice的句子\n",
    "for s in sentences:\n",
    "    cur = [word2idx_cn[n] in encoder_input for n in s[0].split()]\n",
    "    if all(cur):\n",
    "        orginal_sentence = s\n",
    "        break\n",
    "\n",
    "# 打印信息:\n",
    "print(\" 原始句子：\", orginal_sentence) # 打印原始句子\n",
    "print(\" 编码器输入张量的形状：\", encoder_input.shape)  # 打印输入张量形状\n",
    "print(\" 解码器输入张量的形状：\", decoder_input.shape) # 打印输出张量形状\n",
    "print(\" 目标张量的形状：\", target.shape) # 打印目标张量形状\n",
    "print(\" 编码器输入张量：\", encoder_input) # 打印输入张量\n",
    "print(\" 解码器输入张量：\", decoder_input) # 打印输出张量\n",
    "print(\" 目标张量：\", target) # 打印目标张量\n",
    "print(target.view(-1))\n",
    "print(encoder_input.size(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. 定义attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "    def forward(self, encoder_hidden, decoder_hidden):\n",
    "        # 计算点积\n",
    "        raw_weight = torch.bmm(decoder_hidden, encoder_hidden.transpose(1,2))\n",
    "        # 归一化\n",
    "        attn_weight = F.softmax(raw_weight, dim=-1)\n",
    "        # 加权和\n",
    "        attn_output = torch.bmm(attn_weight, encoder_hidden)\n",
    "        return attn_output,attn_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. 定义encoder，重构decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码器结构: Encoder(\n",
      "  (emdedding): Embedding(18, 10)\n",
      "  (rnn): RNN(10, 128, batch_first=True)\n",
      ")\n",
      "解码器结构: Decoder(\n",
      "  (emdedding): Embedding(20, 10)\n",
      "  (rnn): RNN(10, 128, batch_first=True)\n",
      "  (attention): Attention()\n",
      "  (linear): Linear(in_features=256, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# encoder部分跟seq2seq一致\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, cn_input_size, embedding_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.emdedding = nn.Embedding(cn_input_size, embedding_size)\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True)\n",
    "    def forward(self, encoder_input, encoder_hidden):\n",
    "        embedding = self.emdedding(encoder_input)\n",
    "        output, hn = self.rnn(embedding, encoder_hidden)\n",
    "        return output, hn\n",
    "\n",
    "# decoder部分增加attention\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, en_input_size, embedding_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emdedding = nn.Embedding(en_input_size, embedding_size)\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True)\n",
    "        self.attention = Attention()\n",
    "        self.linear = nn.Linear(hidden_size * 2, en_input_size)\n",
    "    def forward(self, decoder_input, encoder_output, encoder_hidden):\n",
    "        embedding = self.emdedding(decoder_input)\n",
    "        output, hn = self.rnn(embedding, encoder_hidden)\n",
    "        attn_output, attn_weight = self.attention(encoder_output, output)\n",
    "        de_output = torch.cat((output, attn_output), dim=-1)\n",
    "        output = self.linear(de_output)\n",
    "        return output, hn, attn_weight\n",
    "\n",
    "# 设置各个层的数量\n",
    "cn_input_size = voc_size_cn\n",
    "en_input_size = voc_size_en\n",
    "embedding_size = 10\n",
    "hidden_size = 128\n",
    "\n",
    "# 创建编码器和解码器\n",
    "encoder = Encoder(cn_input_size, embedding_size, hidden_size)\n",
    "decoder = Decoder(en_input_size, embedding_size, hidden_size)\n",
    "\n",
    "print(\"编码器结构:\", encoder)\n",
    "print(\"解码器结构:\", decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5. 定义seq2seq with attention结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (emdedding): Embedding(18, 10)\n",
      "    (rnn): RNN(10, 128, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (emdedding): Embedding(20, 10)\n",
      "    (rnn): RNN(10, 128, batch_first=True)\n",
      "    (attention): Attention()\n",
      "    (linear): Linear(in_features=256, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, cn_input, cn_hidden, en_input):\n",
    "        output, hidden = self.encoder(cn_input, cn_hidden)\n",
    "        decoder_output, _, attn_weight = self.decoder(en_input, output, hidden)\n",
    "        return decoder_output, attn_weight\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0040 cost = 0.000752\n",
      "Epoch: 0080 cost = 0.000409\n",
      "Epoch: 0120 cost = 0.000142\n",
      "Epoch: 0160 cost = 0.000088\n",
      "Epoch: 0200 cost = 0.000086\n",
      "Epoch: 0240 cost = 0.000063\n",
      "Epoch: 0280 cost = 0.000076\n",
      "Epoch: 0320 cost = 0.000062\n",
      "Epoch: 0360 cost = 0.000037\n",
      "Epoch: 0400 cost = 0.000030\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 400\n",
    "def train_seq2seq(model, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        encoder_input, decoder_input, target = make_data(sentences) # 获得训练数据\n",
    "        encoder_hidden = torch.zeros(1, encoder_input.size(0), hidden_size)# 初始化encoder的隐藏状态\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        output, _ = model(encoder_input, encoder_hidden, decoder_input)\n",
    "        loss = criterion(output.view(-1, voc_size_en), target.view(-1))\n",
    "        if (epoch + 1) % 40 == 0: # 打印损失\n",
    "            print(f\"Epoch: {epoch + 1:04d} cost = {loss:.6f}\")     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "train_seq2seq(model, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 7. 可视化注意力权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
