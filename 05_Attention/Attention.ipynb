{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "在之前的Seq2Seq中简单介绍了注意力机制，其通过在每个时间步中为输入序列中不同位置袋词分配不同的注意力权重。使得模型能够更加灵活地有选择地关注输入序列中的重要部分，从而更好地捕捉上下文相关性，模型的性能也会因此而提高。  \n",
    "\n",
    "在Attention中最重要的就是如何计算得分，下图中列出了三种计算score的方法:\n",
    "<img src=\"./images/score_function.png\" alt=\"examples\" style=\"zoom:30%;\" />  \n",
    "其中最简单的就是点积注意力，所以以点积注意力为例介绍下计算两个张量的点积注意力的步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot-Product Attention\n",
    "\n",
    "点积注意力顾名思义就是计算两个向量的点积:  \n",
    "\n",
    "<img src=\"./images/attention.png\" alt=\"examples\" style=\"zoom:80%;\" />  \n",
    "\n",
    "分步骤介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.创建两个张量: x1和x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5337, -0.4676, -0.2803, -0.6368],\n",
      "         [ 0.4674, -0.6213,  1.5579,  0.2032],\n",
      "         [-0.1544, -0.9061,  0.7596, -0.0978]],\n",
      "\n",
      "        [[ 1.7761,  1.1352,  0.7950,  1.2606],\n",
      "         [ 0.2405,  1.2241, -0.0210, -1.2548],\n",
      "         [-0.2855, -1.1407, -1.2737, -0.9245]]])\n",
      "tensor([[[-1.9028, -1.1113,  0.9790,  1.7046],\n",
      "         [ 0.0688,  1.5732,  0.0434, -0.3994],\n",
      "         [-1.2683,  0.6892,  0.7112,  1.4494],\n",
      "         [-0.1494,  1.1914, -0.1581,  0.4710],\n",
      "         [-1.2611, -0.0855, -1.4043, -0.2682]],\n",
      "\n",
      "        [[ 0.1631, -0.7872,  0.7288,  1.4120],\n",
      "         [ 0.1255, -0.1833,  0.8956, -1.5014],\n",
      "         [-0.6591, -0.4399,  0.4521,  0.7559],\n",
      "         [ 0.9043, -0.0518, -0.5727,  0.3202],\n",
      "         [-0.3043,  0.3897, -0.0329, -0.7944]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x1 = torch.randn(2, 3, 4) # (batch_size, seq_len1,  feature_dim)\n",
    "x2 = torch.randn(2, 5, 4) # (batch_size, seq_len2,  feature_dim)\n",
    "\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2.计算点积, 得到原始权重(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.9028,  0.0688, -1.2683, -0.1494, -1.2611],\n",
      "         [-1.1113,  1.5732,  0.6892,  1.1914, -0.0855],\n",
      "         [ 0.9790,  0.0434,  0.7112, -0.1581, -1.4043],\n",
      "         [ 1.7046, -0.3994,  1.4494,  0.4710, -0.2682]],\n",
      "\n",
      "        [[ 0.1631,  0.1255, -0.6591,  0.9043, -0.3043],\n",
      "         [-0.7872, -0.1833, -0.4399, -0.0518,  0.3897],\n",
      "         [ 0.7288,  0.8956,  0.4521, -0.5727, -0.0329],\n",
      "         [ 1.4120, -1.5014,  0.7559,  0.3202, -0.7944]]])\n"
     ]
    }
   ],
   "source": [
    "# transpose为转置函数\n",
    "# bmm为批量矩阵乘法:((batch_size , M, N),(batch_size, N, P)) -> (batch_size, M, P)\n",
    "# 除了bmm也可以使用matmul函数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
