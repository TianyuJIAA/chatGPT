{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram\n",
    "\n",
    "在N-Gram模型中，通过将文本分割成连续的N个词的组合，来近似地描述词序列的联合概率。假设一个词出现的概率仅依赖于它前面的N-1个词。换句话说，利用有限的上下文信息(N-1)词来近似地预测下一个词的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建一个Bigram字符预测模型\n",
    "整体结构为：\n",
    "\n",
    "<img src=\"./images/BiGram.png\" alt=\"img\" style=\"zoom: 60%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.构建实验语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我喜欢吃苹果', '我喜欢吃香蕉', '她喜欢吃葡萄', '他不喜欢吃香蕉', '他喜欢吃苹果', '她喜欢吃草莓']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个玩具数据集\n",
    "corpus = [ \"我喜欢吃苹果\",\n",
    "        \"我喜欢吃香蕉\",\n",
    "        \"她喜欢吃葡萄\",\n",
    "        \"他不喜欢吃香蕉\",\n",
    "        \"他喜欢吃苹果\",\n",
    "        \"她喜欢吃草莓\"]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2.把句子分成N个\"Gram\"(分词)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单字列表\n",
      "['我', '喜', '欢', '吃', '苹', '果']\n",
      "['我', '喜', '欢', '吃', '香', '蕉']\n",
      "['她', '喜', '欢', '吃', '葡', '萄']\n",
      "['他', '不', '喜', '欢', '吃', '香', '蕉']\n",
      "['他', '喜', '欢', '吃', '苹', '果']\n",
      "['她', '喜', '欢', '吃', '草', '莓']\n"
     ]
    }
   ],
   "source": [
    "#  定义一个分词函数，将文本转换为单个字符的列表\n",
    "def tokenize(text):\n",
    "    return [char for char in text]\n",
    "print(\"单字列表\")\n",
    "for text in corpus:\n",
    "    tokens = tokenize(text)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3.计算每个Bigram在语料库中的词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一些测试\n",
    "l = [1,2,3,4,5]\n",
    "l[0:1]\n",
    "l[0:-1]\n",
    "t = tuple(l[0:-1])\n",
    "print(t)\n",
    "prefix = t[:-1]\n",
    "prefix\n",
    "sum(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram 词频:\n",
      "我: {'喜': 2}\n",
      "喜: {'欢': 6}\n",
      "欢: {'吃': 6}\n",
      "吃: {'苹': 2, '香': 2, '葡': 1, '草': 1}\n",
      "苹: {'果': 2}\n",
      "香: {'蕉': 2}\n",
      "她: {'喜': 2}\n",
      "葡: {'萄': 1}\n",
      "他: {'不': 1, '喜': 1}\n",
      "不: {'喜': 1}\n",
      "草: {'莓': 1}\n"
     ]
    }
   ],
   "source": [
    "# 定义计算N-Gram词频的函数\n",
    "from collections import defaultdict, Counter\n",
    "# defaultdict(Counter) value为Counter类型\n",
    "# Counter是一个无序的容器类型，以字典的键值对形式存储，其中元素作为key，其计数作为value\n",
    "def count_ngrams(corpos, n):\n",
    "    ngrams_count = defaultdict(Counter) # 创建一个字典，存储N-Gram计数\n",
    "    for text in corpos: # 遍历语料库中的每个文本\n",
    "        tokens = tokenize(text) # 对文本进行分词\n",
    "        for i in range(len(tokens) - n + 1): # 遍历分词结果，根据n值生成N个元素\n",
    "            ngram = tuple(tokens[i:i+n]) # 创建一个N-Gram元祖\n",
    "            prefix = ngram[:-1] # 前缀(n - 1)\n",
    "            token = ngram[-1] # 获取N-Gram的目标单字\n",
    "            ngrams_count[prefix][token] += 1 # 更新 N-Gram计数\n",
    "    return ngrams_count\n",
    "bigram_counts = count_ngrams(corpus,2) # 计算 bigram 词频\n",
    "print(\"bigram 词频:\")\n",
    "for prefix, counts in bigram_counts.items():\n",
    "    print(\"{}: {}\".format(\"\".join(prefix),dict(counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4.定义计算 N-Gram 出现概率的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram出现的概率统计:\n",
      "我: {'喜': 1.0}\n",
      "喜: {'欢': 1.0}\n",
      "欢: {'吃': 1.0}\n",
      "吃: {'苹': 0.3333333333333333, '香': 0.3333333333333333, '葡': 0.16666666666666666, '草': 0.16666666666666666}\n",
      "苹: {'果': 1.0}\n",
      "香: {'蕉': 1.0}\n",
      "她: {'喜': 1.0}\n",
      "葡: {'萄': 1.0}\n",
      "他: {'不': 0.5, '喜': 0.5}\n",
      "不: {'喜': 1.0}\n",
      "草: {'莓': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def ngram_prebabilities(ngram_counts):\n",
    "    ngram_probs = defaultdict(Counter) # 存储N-Gram出现的概率\n",
    "    for prefix, tokens_counts in ngram_counts.items():\n",
    "        total_count = sum(tokens_counts.values()) # 计算当前前缀下共有多少个N-Grams\n",
    "        for token, count in tokens_counts.items(): # 遍历每个prefix\n",
    "            ngram_probs[prefix][token] = count / total_count # 计算每个N-Gram出现的概率\n",
    "    return ngram_probs\n",
    "bigram_probs = ngram_prebabilities(bigram_counts)\n",
    "print(\"bigram出现的概率统计:\")\n",
    "for prefix, probs in bigram_probs.items():\n",
    "    print(\"{}: {}\".format(\"\".join(prefix),dict(probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5.根据Bigram的概率统计，定义生成下一个词的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "Counter({'f': 7, 'd': 3, 's': 2, 'a': 1})\n",
      "range(0, 3)\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 测试下max函数\n",
    "d1 = {'a':1,'b':2,'c':3}\n",
    "print(max(d1))\n",
    "c1 = Counter('dsfadsfdfffff')\n",
    "print(c1)\n",
    "max(c1, key=c1.get) # Count与dict有区别，如果指定key，输出counter中key最大的值而不是value\n",
    "print(range(3))\n",
    "for i in range(len(d1)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_next_token(prefix, bigram_probs):\n",
    "    if not prefix in bigram_probs:\n",
    "        return None # 如果prefix不在dict中，返回none\n",
    "    next_token_probs = bigram_probs[prefix]\n",
    "    next_token = max(next_token_probs, key=next_token_probs.get) # 返回出现概率最高的词汇\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6.定义生成连续文本的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(prefix, ngram_probs, n, length=6):\n",
    "    tokens = list(prefix)\n",
    "    for _ in range(length - len(prefix)):\n",
    "        next_token = gen_next_token(tuple(tokens[-(n-1):]), ngram_probs)\n",
    "        if not next_token:\n",
    "            break\n",
    "        tokens.append(next_token)\n",
    "    return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "text: 我喜欢吃苹果\n"
     ]
    }
   ],
   "source": [
    "# 给定一个prefix，输出完整的句子\n",
    "prefix = '我'\n",
    "n = 2\n",
    "print(\"\\ntext:\", gen_text(prefix, bigram_probs, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
